# -*- coding: utf-8 -*-
import multiprocessing
import time
import logging
import os
from collections import deque
import pickle
import threading

import numpy as np
import torch
from mpmath import mp

from pytorch_net import PolicyValueNet
from zip_array import zip_array_fast, recovery_array_fast,compress_game_data,decompress_game_data

# ä»å·²æœ‰æ¨¡å—å¯¼å…¥
from game import Board, Game, move_id2move_action, move_action2move_id, flip_map
from mcts import MCTSPlayer
from config import CONFIG

# Redis æ”¯æŒ
if CONFIG['use_redis']:
    import redis

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# æ—¥å¿—è®¾ç½®
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("MergeData")

# å…¨å±€é”ç”¨äºæ‰“å°åŒæ­¥
print_lock = multiprocessing.Lock()

# æ¨¡å‹è·¯å¾„
MODEL_PATH = CONFIG['pytorch_model_path']


# è‡ªå®šä¹‰ CollectPipeline ç±»ï¼ˆè¿›ç¨‹å®‰å…¨ï¼‰
class ProcessSafeCollectPipeline:

    def __init__(self, model, process_id):
        self.process_id = process_id
        self.board = Board()
        self.game = Game(self.board)
        self.temp = 1e-3
        self.n_playout = CONFIG.get('play_out', 400)  # é»˜è®¤å€¼ 400
        self.c_puct = CONFIG.get('c_puct', 5)
        self.buffer_size = CONFIG.get('buffer_size', 100000)
        self.iters = 0
        self.temp_play_data = []  # ä¸´æ—¶ç¼“å­˜æœªå®Œæˆçš„å¯¹å±€æ•°æ®
        self.nums = [] #  è®°å½•æ¯å±€æ­¥æ•° * 2ï¼ˆåŒ…æ‹¬é•œåƒï¼‰
        self.use_compression = CONFIG.get('use_data_compression', False)  # é»˜è®¤ä¸å¯ç”¨


        if CONFIG['use_redis']:
            self.redis_client = redis.Redis(
                host=CONFIG['redis_host'],
                port=CONFIG['redis_port'],
                db=CONFIG['redis_db']
            )
        else:
            self.data_buffer = deque(maxlen=self.buffer_size)
            self.temp_play_data = []

        # åŠ è½½ä¼ å…¥çš„å…±äº«æ¨¡å‹
        self.policy_value_net = model
        if CONFIG['use_frame'] == 'pytorch' and torch.cuda.is_available():
            self.policy_value_net.device = torch.device('cuda')

        self.mcts_player = MCTSPlayer(
            self.policy_value_net.policy_value_fn,
            c_puct=self.c_puct,
            n_playout=self.n_playout,
            is_selfplay=1
        )

        # å°è¯•ä»ç£ç›˜/Redis æ¢å¤æ•°æ®
        self.load_data_buffer()

    def load_data_buffer(self):
        data_key = f"data/data_buffer_process_{self.process_id}"
        if CONFIG['use_redis']:
            meta = self.redis_client.get(data_key)
            if meta:
                try:
                    meta_dict = pickle.loads(meta)
                    self.data_buffer = deque(meta_dict['data_buffer'], maxlen=self.buffer_size)
                    self.iters = meta_dict.get('iters', 0)
                    self.nums = meta_dict.get('nums', [])
                    logger.info(f"[è¿›ç¨‹ {self.process_id}] ä» Redis æ¢å¤æ•°æ®ï¼Œè¿­ä»£æ¬¡æ•°: {self.iters}")
                except Exception as e:
                    logger.error(f"[è¿›ç¨‹ {self.process_id}] Redis æ•°æ®åŠ è½½å¤±è´¥: {e}")
        else:
            filename = f"{data_key}.pkl"
            if os.path.exists(filename):
                try:
                    with open(filename, 'rb') as f:
                        data_dict = pickle.load(f)

                    # å¦‚æœå¯ç”¨äº†å‹ç¼©ï¼Œåˆ™å¯¹æ•°æ®è¿›è¡Œè§£å‹
                    loaded_data = data_dict.get('data_buffer', [])
                    if self.use_compression and loaded_data and isinstance(loaded_data[0], bytes):
                        decompressed_data = []
                        for item in loaded_data:
                            decompressed = decompress_game_data(item)
                            decompressed_data.extend(decompressed)
                        loaded_data = decompressed_data

                    self.data_buffer = deque(loaded_data, maxlen=self.buffer_size)
                    self.iters = data_dict.get('iters', 0)
                    self.nums = data_dict.get('nums', [])
                    logger.info(f"[è¿›ç¨‹ {self.process_id}] ä»æœ¬åœ°æ–‡ä»¶ {filename} æ¢å¤æ•°æ®ï¼Œè¿­ä»£æ¬¡æ•°: {self.iters}")
                except Exception as e:
                    logger.error(f"[è¿›ç¨‹ {self.process_id}] æ–‡ä»¶ {filename} åŠ è½½å¤±è´¥: {e}")


    def get_equi_data(self, play_data):
        extend_data = []
        for state, mcts_prob, winner in play_data:
            # åŸå§‹æ•°æ®
            extend_data.append((state, mcts_prob, winner))

            # æ°´å¹³ç¿»è½¬åçš„æ•°æ®
            state = state.transpose([1, 2, 0])  # CHW -> HWC
            state_flip = np.zeros_like(state)
            for i in range(10):
                for j in range(9):
                    state_flip[i][j] = state[i][8 - j]
            state_flip = state_flip.transpose([2, 0, 1])  # HWC -> CHW

            mcts_prob_flip = [0] * len(mcts_prob)
            for i in range(len(mcts_prob_flip)):
                action = move_id2move_action[i]
                flipped_action = flip_map(action)
                flipped_id = move_action2move_id.get(flipped_action, None)
                if flipped_id is not None:
                    mcts_prob_flip[i] = mcts_prob[flipped_id]

            extend_data.append((state_flip, mcts_prob_flip, winner))
        return extend_data

    def run(self, logger=None):
        try:
            while True:
                self.collect_selfplay_data(logger=logger)
                time.sleep(1)  # é¿å… CPU å ç”¨è¿‡é«˜
        except KeyboardInterrupt:
            logger.info(f"[è¿›ç¨‹ {self.process_id}] æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆæ•°æ®...")

    def collect_selfplay_data(self, logger=None):
        try:
            winner, play_data = self.game.start_self_play(self.mcts_player, temp=self.temp, is_shown=True, logger=logger)
            play_data = list(play_data)
            episode_len = len(play_data)

            # æ•°æ®å¢å¼º
            extended_data = self.get_equi_data(play_data)

            # å…ˆæš‚å­˜åœ¨ä¸´æ—¶å˜é‡ä¸­
            self.temp_play_data = extended_data


            if CONFIG['use_redis']:
                data_key = f'train_data:{self.process_id}'
                data_dict = {
                    'data_buffer': self.temp_play_data,
                    'iters': self.iters + 1,
                    'nums': self.nums
                }
                try:
                    self.redis_client.setex(data_key, 3600, pickle.dumps(data_dict))
                    logger.info(f"[è¿›ç¨‹ {self.process_id}] å·²å†™å…¥ Redis: {data_key}")
                except Exception as e:
                    logger.error(f"[è¿›ç¨‹ {self.process_id}] Redis å†™å…¥å¤±è´¥: {e}")
            else:
                # ä»…åœ¨å®Œæ•´å±€ç»“æŸåæ‰å†™å…¥ä¸» buffer
                self.data_buffer.extend(self.temp_play_data)
                self.iters += 1
                self.nums.append(episode_len*2)

            self.save_to_disk()

            logger.info(f"[è¿›ç¨‹ {self.process_id}] ç¬¬ {self.iters} å±€ç»“æŸï¼Œæ€»æ­¥æ•°: {episode_len}, èƒœè€…: {winner}")

        except Exception as e:
            logger.error(f"[è¿›ç¨‹ {self.process_id}] å¯¹å±€ä¸­æ–­æˆ–å‡ºé”™: {e}")
            print(f"[è¿›ç¨‹ {self.process_id}] âš ï¸ å¯¹å±€ä¸­æ–­ï¼Œæ”¾å¼ƒå½“å‰æœªå®Œæˆçš„æ•°æ®")
            self.temp_play_data.clear()  # æ¸…é™¤ä¸´æ—¶æ•°æ®


    def save_to_disk(self):
        if not CONFIG['use_redis']:
            data_key = f"data/data_buffer_process_{self.process_id}"
            filename = f"{data_key}.pkl"
            temp_filename = f"{filename}.tmp"

            # å¦‚æœå¯ç”¨äº†å‹ç¼©ï¼Œåˆ™å¯¹æ•°æ®è¿›è¡Œå‹ç¼©
            if self.use_compression:
                compressed_data_list = []
                for item in self.data_buffer:
                    compressed_data = compress_game_data([item])  # å•æ¡æ•°æ®åŒ…è£…æˆåˆ—è¡¨
                    compressed_data_list.extend(compressed_data)
                data_to_save = {
                    'data_buffer': compressed_data_list,
                    'iters': self.iters,
                    'nums': self.nums,
                }
            else:
                data_to_save = {
                    'data_buffer': list(self.data_buffer),
                    'iters': self.iters,
                    'nums': self.nums,
                }

            try:
                with open(temp_filename, 'wb') as f:
                    pickle.dump(data_to_save, f)
                os.replace(temp_filename, filename)
            except Exception as e:
                logger.error(f"[è¿›ç¨‹ {self.process_id}] ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
                if os.path.exists(temp_filename):
                    os.remove(temp_filename)



# ä¸»è¿›ç¨‹åˆå¹¶
def merge_and_cleanup_data_buffers(output_path, num_processes, buffer_size=100000):
    merged_buffer = deque(maxlen=buffer_size)
    total_iters = 0
    skipped_episodes = 0
    step_nums_unique = []

    # Step 1: åŠ è½½å·²æœ‰ä¸»æ•°æ®ï¼Œå¹¶æå–å…¶ episode å“ˆå¸Œå€¼ç”¨äºåç»­å¯¹æ¯”
    existing_episodes = set()
    if os.path.exists(output_path):
        try:
            with open(output_path, 'rb') as f:
                existing_data = pickle.load(f)
            for item in existing_data.get('data_buffer', []):
                merged_buffer.append(item)

            # æå–å†å²æ•°æ®ä¸­çš„ episodes å¹¶å“ˆå¸Œå­˜å‚¨
            play_index = 0
            play_data = existing_data.get('data_buffer', [])
            step_nums = existing_data.get('nums', [])

            for step_num in step_nums:
                episode_data = list(play_data)[play_index: play_index + step_num]
                if episode_data:
                    hashable = tuple((tuple(s.flatten()), tuple(mp), w) for s, mp, w in episode_data)
                    existing_episodes.add(hashable)
                play_index += step_num

            logger.info(f"âœ… å·²åŠ è½½å†å²æ•°æ® {len(existing_data.get('data_buffer', []))} æ¡")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")

    # Step 2: æ”¶é›†æ‰€æœ‰å­è¿›ç¨‹æ•°æ®åˆ°ä¸€ä¸ªå…¨å±€ç¼“å†²åŒº
    global_play_data = []
    global_step_nums = []
    total_iters = 0

    if CONFIG['use_redis']:
        redis_client = redis.Redis(
            host=CONFIG['redis_host'],
            port=CONFIG['redis_port'],
            db=CONFIG['redis_db']
        )

        for pid in range(num_processes):
            data_key = f'train_data:{pid}'
            item = redis_client.get(data_key)
            if item:
                try:
                    data_dict = pickle.loads(item)
                    play_data = data_dict.get('data_buffer', [])
                    step_nums = data_dict.get('nums', [])
                    pid_iters = data_dict.get('iters', 0)

                    global_play_data.extend(play_data)
                    global_step_nums.extend(step_nums)
                    total_iters += pid_iters

                    redis_client.delete(data_key)
                    logger.info(f"[è¿›ç¨‹ {pid}] æ•°æ®å·²ä» Redis å–å‡º")
                except Exception as e:
                    logger.error(f"âŒ Redis æ•°æ®ååºåˆ—åŒ–å¤±è´¥: {e}")
    else:
        for pid in range(num_processes):
            filename = f"data/data_buffer_process_{pid}.pkl"
            if not os.path.exists(filename):
                continue
            try:
                with open(filename, 'rb') as f:
                    data_dict = pickle.load(f)

                play_data = data_dict.get('data_buffer', [])
                step_nums = data_dict.get('nums', [])
                pid_iters = data_dict.get('iters', 0)

                # å¦‚æœæ˜¯å‹ç¼©æ•°æ®ï¼Œåˆ™è§£å‹
                if CONFIG.get('use_data_compression', False) :
                    decompressed = decompress_game_data(play_data)

                global_play_data.extend(decompressed)
                global_step_nums.extend(step_nums)
                total_iters += pid_iters

                logger.info(f"[è¿›ç¨‹ {pid}] æ•°æ®å·²ä»æœ¬åœ°å–å‡º")
            except Exception as e:
                logger.error(f"âŒ åˆå¹¶ {filename} æ—¶å‡ºé”™: {str(e)}")


    # Step 3: æ ¹æ® step_nums åˆ’åˆ† episode
    episodes = []
    play_index = 0
    for step_num in global_step_nums:
        episode_data = list(global_play_data)[play_index: play_index + step_num]
        if episode_data:
            episodes.append(episode_data)
        play_index += step_num

    # Step 4: å­æ•°æ®é›†å†…éƒ¨å»é‡
    seen_episodes = set()
    unique_episodes_from_processes = []

    for ep_idx, episode in enumerate(episodes):
        hashable_episode = tuple(
            (tuple(state.flatten()), tuple(mcts_prob), winner)
            for state, mcts_prob, winner in episode
        )
        if hashable_episode not in seen_episodes:
            seen_episodes.add(hashable_episode)
            unique_episodes_from_processes.append(episode)
            step_nums_unique.append(global_step_nums[ep_idx])
        else:
            skipped_episodes += 1

    logger.info(f"ğŸ“Œ å­æ•°æ®é›†å†…éƒ¨å»é‡å®Œæˆï¼Œå…±ä¿ç•™ {len(unique_episodes_from_processes)} å±€")

    # Step 5: ä¸å·²æœ‰ä¸»æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œæ’é™¤é‡å¤çš„å±€
    final_unique_episodes = []

    for episode in unique_episodes_from_processes:
        hashable = tuple(
            (tuple(state.flatten()), tuple(mcts_prob), winner)
            for state, mcts_prob, winner in episode
        )
        if hashable not in existing_episodes:
            final_unique_episodes.append(episode)
        else:
            skipped_episodes += 1

    logger.info(f"ğŸ“Œ ä¸ä¸»æ•°æ®å¯¹æ¯”åï¼Œå…±ä¿ç•™ {len(final_unique_episodes)} å±€æ–°æ•°æ®")

    # Step 6: å†™å…¥æœ€ç»ˆæ•°æ®
    for episode in final_unique_episodes:
        # å‹ç¼©æ•°æ®
        compressed_data = compress_game_data(episode)
        merged_buffer.extend(compressed_data)

    # æ„é€ è¾“å‡ºç»“æ„
    merged_data = {
        'data_buffer': list(merged_buffer),
        'iters': total_iters,
        # 'nums': step_nums_unique,
    }

    with open(output_path, 'wb') as f:
        pickle.dump(merged_data, f)

    logger.info(f"âœ… æ‰€æœ‰è¿›ç¨‹æ•°æ®å·²è¿½åŠ åˆå¹¶è‡³ {output_path}")
    logger.info(f"ğŸš« å…±è·³è¿‡ {skipped_episodes} å±€é‡å¤æ•°æ®")



# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logger(process_id):
    logger = logging.getLogger(f"selfplay-{process_id}")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()  # æ¸…é™¤æ—§ handler
    file_handler = logging.FileHandler(os.path.join(LOG_DIR, f"selfplay_process_{process_id}.log"), encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(message)s')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    return logger


# å¤šè¿›ç¨‹å…¥å£å‡½æ•°
def run_pipeline(process_id, shared_model):
    logger = setup_logger(process_id)
    logger.info(f"__________________________________________")
    logger.info(f"è¿›ç¨‹ {process_id} å¯åŠ¨...")

    pipeline = ProcessSafeCollectPipeline(model=shared_model, process_id=process_id)
    logger.info(f"[è¿›ç¨‹ {process_id}] åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æŒç»­é‡‡é›†æ•°æ®")

    pipeline.run(logger=logger)

    logger.info(f"è¿›ç¨‹ {process_id} ç»“æŸ.")
    with print_lock:
        print(f"âœ… è¿›ç¨‹ {process_id} å·²ç»“æŸ")


# åŠ¨æ€è°ƒæ•´è¿›ç¨‹ç®¡ç†å™¨
def dynamic_process_manager(shared_model, target_num_processes):
    active_processes = []

    while True:
        current_num = len(active_processes)
        desired_num = CONFIG.get('num_processes', 4)

        if desired_num > current_num:
            # å¯åŠ¨æ–°è¿›ç¨‹
            for pid in range(current_num, desired_num):
                p = mp.Process(target=run_pipeline, args=(pid, shared_model))
                p.start()
                active_processes.append(p)
                print(f"â• æ–°å¢è¿›ç¨‹ PID={pid}")
        elif desired_num < current_num:
            # ç»ˆæ­¢å¤šä½™çš„è¿›ç¨‹
            for p in active_processes[desired_num:]:
                p.terminate()
                p.join()
            active_processes = active_processes[:desired_num]
            print(f"â– å‡å°‘è¿›ç¨‹è‡³ {desired_num}")

        time.sleep(10)  # æ¯éš”10ç§’æ£€æŸ¥ä¸€æ¬¡é…ç½®å˜åŒ–


if __name__ == "__main__":
    multiprocessing.freeze_support()  # Windows æ”¯æŒ
    NUM_PROCESSES = CONFIG['num_processes']
    interval = CONFIG['interval']

    print("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
    try:
        policy_value_net = PolicyValueNet(model_file=MODEL_PATH)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼š",  MODEL_PATH)
    except Exception as e:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•åˆå§‹åŒ–æ–°æ¨¡å‹")
        policy_value_net = PolicyValueNet()
        print("âœ… æˆåŠŸåˆå§‹åŒ–ç©ºç™½æ¨¡å‹")

    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨GPU
    device = 'cuda' if torch.cuda.is_available() and CONFIG['use_frame'] == 'pytorch' else 'cpu'
    print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device.upper()}")

    # å…ˆåˆå¹¶å†å²æ•°æ®
    OUTPUT_PATH = CONFIG['train_data_buffer_path']

    # ä½¿ç”¨ spawn æ–¹å¼å¯åŠ¨å¤šè¿›ç¨‹ï¼ˆé€‚ç”¨äº GPUï¼‰
    ctx = multiprocessing.get_context('spawn')
    Process = ctx.Process

    processes = []
    try:
        for pid in range(NUM_PROCESSES):
            p = Process(target=run_pipeline, args=(pid, policy_value_net))  # åªä¼ ä¸¤ä¸ªå‚æ•°
            processes.append(p)
            p.start()

        for p in processes:
            p.join()
    except KeyboardInterrupt:
        print("ğŸ›‘ ä¸»è¿›ç¨‹æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå¼€å§‹åˆå¹¶æ•°æ®...")
        merge_and_cleanup_data_buffers(OUTPUT_PATH, NUM_PROCESSES)
        print("âœ… æ•°æ®åˆå¹¶å®Œæˆ")
