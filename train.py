import os
import random
import time

import numpy as np
import pickle
from collections import deque, defaultdict

from config import CONFIG
from game import Game, Board
from mcts import MCTSPlayer
from mcts_pure import MCTS_Pure
from zip_array import decompress_game_data
if CONFIG['use_frame'] == 'pytorch':
    from pytorch_net import PolicyValueNet
elif CONFIG['use_frame'] == 'paddle':
    from paddle_net import PolicyValueNet
else:
    raise NotImplementedError("æš‚ä¸æ”¯æŒæ‰€é€‰æ¡†æž¶")

class TrainPipeline:

    def __init__(self, init_model=None):
        # è®­ç»ƒå‚æ•°
        self.board = Board()
        self.game = Game(self.board)
        self.n_playout = CONFIG['play_out']
        self.lr_multiplier = 1  # å­¦ä¹ çŽ‡è‡ªé€‚åº”è°ƒæ•´
        self.learn_rate = CONFIG['learn_rate',1e-3]
        self.batch_size = CONFIG['batch_size']
        self.c_puct = CONFIG['c_puct']
        self.epochs = CONFIG['epochs']
        self.kl_targ = CONFIG['kl_targ',0.02]  # klæ•£åº¦æŽ§åˆ¶
        self.check_freq = CONFIG['check_freq',100] # ä¿å­˜æ¨¡åž‹çš„é¢‘çŽ‡
        self.game_batch_num = CONFIG['game_batch_num']  # è®­ç»ƒæ›´æ–°çš„æ¬¡æ•°
        self.use_compression = CONFIG.get('use_data_compression', False)
        self.temp = 1.0
        self.best_win_ratio = 0.0
        self.pure_mcts_playout_num = 500

        # åˆå§‹åŒ–æ•°æ®ç¼“å†²åŒº
        self.data_buffer = deque(maxlen=CONFIG['buffer_size'])

        # åŠ è½½æ¨¡åž‹
        if init_model:
            try:
                self.policy_value_net = PolicyValueNet(model_file=init_model)
                print('âœ… å·²åŠ è½½ä¸Šæ¬¡æ¨¡åž‹:', init_model)
            except Exception as e:
                print('âŒ æ¨¡åž‹åŠ è½½å¤±è´¥:', e)
                self.policy_value_net = PolicyValueNet()
        else:
            print('ðŸ†• ä»Žé›¶å¼€å§‹è®­ç»ƒ')
            self.policy_value_net = PolicyValueNet()
    def run_continuously(self):
        """æŒç»­è®­ç»ƒæ¨¡å¼ï¼Œæ¯éš”ä¸€æ®µæ—¶é—´æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®åŠ å…¥"""
        try:
            while True:
                if not CONFIG['use_redis']:
                    # ä»Žç£ç›˜é‡æ–°åŠ è½½æ•°æ®
                    try:
                        with open(CONFIG['train_data_buffer_path'], 'rb') as f:
                            data_dict = pickle.load(f)
                        raw_data = data_dict['data_buffer']
                        if self.use_compression:
                            new_data = decompress_game_data(raw_data)
                        else:
                            new_data = raw_data

                        self.data_buffer.extend(new_data)
                        print(f"ðŸ“¥ å·²åŠ è½½ {len(new_data)} æ¡æ–°æ•°æ®")

                    except Exception as e:
                        print(f"âŒ åŠ è½½æ–°æ•°æ®å¤±è´¥: {e}")
                else:
                    # ä»Ž Redis èŽ·å–æ–°æ•°æ®
                    pass  # ç•¥ï¼Œæ ¹æ®ä½ çš„ Redis å®žçŽ°è¡¥å……

                # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿå¹¶è®­ç»ƒ
                if len(self.data_buffer) > self.batch_size:
                    print("ðŸ‹ï¸ å¼€å§‹æœ¬è½®è®­ç»ƒ")
                    self.policy_update()
                else:
                    print("â³ æ•°æ®ä¸è¶³ï¼Œç­‰å¾…é‡‡é›†...")

                time.sleep(10)  # æ¯éš”10ç§’æ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print("\n\rðŸ›‘ è®­ç»ƒå·²æ‰‹åŠ¨ç»ˆæ­¢")
    def policy_evaluate(self, n_games=10):
        """
        Evaluate the trained policy by playing against the pure MCTS player
        Note: this is only for monitoring the progress of training
        """
        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,
                                         c_puct=self.c_puct,
                                         n_playout=self.n_playout)
        pure_mcts_player = MCTS_Pure(c_puct=5,
                                     n_playout=self.pure_mcts_playout_num)
        win_cnt = defaultdict(int)
        for i in range(n_games):
            winner = self.game.start_play(current_mcts_player,
                                          pure_mcts_player,
                                          start_player=i % 2 + 1,
                                          is_shown=1)
            win_cnt[winner] += 1
        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games
        print("num_playouts:{}, win: {}, lose: {}, tie:{}".format(
                self.pure_mcts_playout_num,
                win_cnt[1], win_cnt[2], win_cnt[-1]))
        return win_ratio


    def policy_update(self):
        """æ›´æ–°ç­–ç•¥ä»·å€¼ç½‘ç»œ"""
        mini_batch = random.sample(self.data_buffer, self.batch_size)
        state_batch = [data[0] for data in mini_batch]
        mcts_probs_batch = [data[1] for data in mini_batch]
        winner_batch = [data[2] for data in mini_batch]

        state_batch = np.array(state_batch).astype('float32')
        mcts_probs_batch = np.array(mcts_probs_batch).astype('float32')
        winner_batch = np.array(winner_batch).astype('float32')

        old_probs, old_v = self.policy_value_net.policy_value(state_batch)

        for i in range(self.epochs):
            loss, entropy = self.policy_value_net.train_step(
                state_batch, mcts_probs_batch, winner_batch,
                self.learn_rate * self.lr_multiplier
            )
            new_probs, new_v = self.policy_value_net.policy_value(state_batch)

            kl = np.mean(np.sum(old_probs * (
                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)), axis=1))
            if kl > self.kl_targ * 4:
                break

        # è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ çŽ‡
        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:
            self.lr_multiplier /= 1.5
        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:
            self.lr_multiplier *= 1.5

        print(f"KL: {kl:.5f}, LR Multiplier: {self.lr_multiplier:.3f}, Loss: {loss}, Entropy: {entropy}")
        return loss, entropy

    def run(self):
        """å¼€å§‹è®­ç»ƒ"""
        train_data_path = CONFIG['train_data_buffer_path']

        # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®
        print(f"ðŸ“‚ æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {train_data_path}")
        with open(train_data_path, 'rb') as f:
            data_dict = pickle.load(f)

        raw_data = data_dict['data_buffer']
        if self.use_compression:
            self.data_buffer.extend(decompress_game_data(raw_data))
        else:
            self.data_buffer.extend(raw_data)

        print(f"ðŸ“¥ æˆåŠŸåŠ è½½ {len(self.data_buffer)} æ¡æ•°æ®")

        # å¼€å§‹è®­ç»ƒ
        for i in range(self.game_batch_num):
            if len(self.data_buffer) < self.batch_size:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                break

            print(f"ðŸ‹ï¸ ç¬¬ {i+1} è½®è®­ç»ƒå¼€å§‹")
            loss, entropy = self.policy_update()

            # å®šæœŸä¿å­˜æ¨¡åž‹
            if (i + 1) % self.check_freq == 0:
                model_path = f'models/current_policy_batch_{i+1}.model'
                self.policy_value_net.save_model(model_path)
                print(f"ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜è‡³: {model_path}")

        # æœ€ç»ˆä¿å­˜æ¨¡åž‹
        final_model_path = CONFIG.get('final_model_path', 'models/final_policy.model')
        self.policy_value_net.save_model(final_model_path)
        print(f"ðŸ è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡åž‹ä¿å­˜è‡³: {final_model_path}")



if __name__ == '__main__':
    init_model = CONFIG.get('init_model_path', None)
    training_pipeline = TrainPipeline(init_model=init_model)

    if CONFIG['collect_and_train']:
        print("ðŸ”„ æ­£åœ¨åŒæ—¶è¿›è¡Œé‡‡é›†å’Œè®­ç»ƒ...")
        from collect_multi_thread import run_pipeline, dynamic_process_manager
        import threading

        # å¯åŠ¨é‡‡é›†è¿›ç¨‹ç®¡ç†å™¨ä½œä¸ºåŽå°çº¿ç¨‹
        def start_collector():
            shared_model = PolicyValueNet(model_file=init_model)
            dynamic_process_manager(shared_model, num_processes=CONFIG['num_processes'])

        collector_thread = threading.Thread(target=start_collector, daemon=True)
        collector_thread.start()

        # å¼€å§‹è®­ç»ƒä¸»å¾ªçŽ¯ï¼ˆå¯å®šæœŸä»Žç£ç›˜/Redis èŽ·å–æ–°æ•°æ®ï¼‰
        training_pipeline.run_continuously()
    else:
        print("ðŸ†• åªè¿›è¡Œè®­ç»ƒï¼Œä¸å¯åŠ¨é‡‡é›†")
        training_pipeline.run()
