# -*- coding: utf-8 -*-

import os
import random
import time
import numpy as np
import pickle
from collections import deque, defaultdict

from config import CONFIG
from game import Game, Board
from mcts import MCTSPlayer
from mcts_pure import MCTS_Pure
from zip_array import decompress_game_data

# æ•°æ®æœåŠ¡æ¨¡å—ï¼ˆç»Ÿä¸€ Redis / æ–‡ä»¶ï¼‰
from data_service import DataManagementService

# æ ¹æ®é…ç½®é€‰æ‹©ç½‘ç»œæ¡†æ¶
if CONFIG['use_frame'] == 'pytorch':
    from pytorch_net import PolicyValueNet
elif CONFIG['use_frame'] == 'paddle':
    from paddle_net import PolicyValueNet
else:
    raise NotImplementedError("æš‚ä¸æ”¯æŒæ‰€é€‰æ¡†æ¶")


class TrainPipeline:

    def __init__(self, init_model=None):
        # æ¸¸æˆç¯å¢ƒåˆå§‹åŒ–
        self.board = Board()
        self.game = Game(self.board)

        # è®­ç»ƒå‚æ•°
        self.n_playout = CONFIG.get('play_out', 400)  # MCTS æœç´¢æ¬¡æ•°
        self.lr_multiplier = 1.0  # å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´å› å­
        self.learn_rate = CONFIG.get('learn_rate', 1e-3)
        self.batch_size = CONFIG.get('batch_size', 512)
        self.c_puct = CONFIG.get('c_puct', 5)
        self.epochs = CONFIG.get('epochs', 5)
        self.kl_targ = CONFIG.get('kl_targ', 0.02)  # KLæ•£åº¦ç›®æ ‡
        self.check_freq = CONFIG.get('check_freq', 100)  # æ¨¡å‹ä¿å­˜é¢‘ç‡
        self.game_batch_num = CONFIG.get('game_batch_num', 1000)  # æ€»è®­ç»ƒè½®æ•°
        self.use_compression = CONFIG.get('use_data_compression', False)

        # åˆå§‹åŒ–æ•°æ®æœåŠ¡
        self.data_service = DataManagementService()
        self.data_buffer = self.data_service.data_buffer  # å…±äº«ç¼“å†²åŒº

        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰
        self.data_service.checkpoint = self.data_service.load_checkpoint()
        self.iters = self.data_service.checkpoint.get('iters', 0)
        self.model_path = self.data_service.checkpoint.get('model_path', None)
        self.best_win_ratio = 0.0
        self.pure_mcts_playout_num = 500

        # åŠ è½½æ¨¡å‹
        if self.model_path:
            try:
                self.policy_value_net = PolicyValueNet(model_file=self.model_path)
                print(f'âœ… å·²ä»ä¸Šæ¬¡æ¨¡å‹ç»§ç»­è®­ç»ƒ: {self.model_path}')
            except Exception as e:
                print(f'âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}')
                self.policy_value_net = PolicyValueNet()
        else:
            print('ğŸ†• ä»é›¶å¼€å§‹è®­ç»ƒ')
            self.policy_value_net = PolicyValueNet()

    def run_continuously(self):
        """æŒç»­è®­ç»ƒæ¨¡å¼ï¼šæ¯éš”ä¸€æ®µæ—¶é—´æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®åŠ å…¥"""
        try:
            while True:
                # åˆ·æ–°æ•°æ®
                new_data = self.data_service.refresh_data()
                print(f"ğŸ”„ å½“å‰æ•°æ®ç¼“å­˜å¤§å°: {len(self.data_buffer)}")

                # å¦‚æœæ•°æ®è¶³å¤Ÿåˆ™è®­ç»ƒ
                if len(self.data_buffer) >= self.batch_size:
                    print("ğŸ‹ï¸ å¼€å§‹æœ¬è½®è®­ç»ƒ")
                    loss, entropy = self.policy_update()
                    self.iters += 1

                    # å®šæœŸä¿å­˜æ¨¡å‹å’Œæ£€æŸ¥ç‚¹
                    if self.iters % self.check_freq == 0:
                        model_path = f'models/current_policy_iter_{self.iters}.model'
                        self.policy_value_net.save_model(model_path)
                        self.data_service.save_checkpoint(self.iters, model_path)
                        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

                        # æ¯éš”ä¸€å®šè¿­ä»£è¯„ä¼°èƒœç‡
                        win_ratio = self.policy_evaluate(n_games=5)
                        if win_ratio > self.best_win_ratio:
                            best_model_path = f'models/best_policy_iter_{self.iters}.model'
                            self.policy_value_net.save_model(best_model_path)
                            self.best_win_ratio = win_ratio
                            print(f"ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°ï¼Œèƒœç‡: {win_ratio:.2f}")
                else:
                    print("â³ æ•°æ®ä¸è¶³ï¼Œç­‰å¾…é‡‡é›†...")

                time.sleep(10)  # æ¯éš”10ç§’æ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            print("\n\rğŸ›‘ è®­ç»ƒå·²æ‰‹åŠ¨ç»ˆæ­¢ï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            final_model_path = CONFIG.get('pytorch_model_path', 'models/final_policy.model')
            self.policy_value_net.save_model(final_model_path)
            self.data_service.save_checkpoint(self.iters, final_model_path)
            print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")

    def policy_evaluate(self, n_games=10):
        """
        å¯¹æŠ—çº¯ MCTS ç©å®¶è¯„ä¼°ç­–ç•¥æ€§èƒ½
        """
        current_mcts_player = MCTSPlayer(
            self.policy_value_net.policy_value_fn,
            c_puct=self.c_puct,
            n_playout=self.n_playout
        )
        pure_mcts_player = MCTS_Pure(c_puct=5, n_playout=self.pure_mcts_playout_num)

        win_cnt = defaultdict(int)
        for i in range(n_games):
            winner = self.game.start_play(
                current_mcts_player,
                pure_mcts_player,
                start_player=i % 2 + 1,
                is_shown=1
            )
            win_cnt[winner] += 1

        win_ratio = 1.0 * (win_cnt[1] + 0.5 * win_cnt[-1]) / n_games
        print(f"ğŸ® èƒœç‡: {win_ratio:.2f} (Win: {win_cnt[1]}, Lose: {win_cnt[2]}, Tie: {win_cnt[-1]})")
        return win_ratio

    def policy_update(self):
        """
        æ›´æ–°ç­–ç•¥ä»·å€¼ç½‘ç»œ
        """
        if len(self.data_buffer) < self.batch_size:
            raise ValueError("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ")

        mini_batch = random.sample(self.data_buffer, self.batch_size)
        state_batch = [data[0] for data in mini_batch]
        mcts_probs_batch = [data[1] for data in mini_batch]
        winner_batch = [data[2] for data in mini_batch]

        state_batch = np.array(state_batch).astype('float32')
        mcts_probs_batch = np.array(mcts_probs_batch).astype('float32')
        winner_batch = np.array(winner_batch).astype('float32')

        old_probs, old_v = self.policy_value_net.policy_value(state_batch)

        for i in range(self.epochs):
            loss, entropy = self.policy_value_net.train_step(
                state_batch, mcts_probs_batch, winner_batch,
                self.learn_rate * self.lr_multiplier
            )
            new_probs, new_v = self.policy_value_net.policy_value(state_batch)

            kl = np.mean(np.sum(old_probs * (
                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)), axis=1))
            if kl > self.kl_targ * 4:
                break

        # è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:
            self.lr_multiplier /= 1.5
        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:
            self.lr_multiplier *= 1.5

        print(f"ğŸ“Š KL: {kl:.5f}, LR Multiplier: {self.lr_multiplier:.3f}, Loss: {loss:.4f}, Entropy: {entropy:.4f}")
        return loss, entropy

    def run(self):
        """
        å•æ¬¡è®­ç»ƒæµç¨‹ï¼ˆéæŒç»­è®­ç»ƒï¼‰
        """
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®: {self.data_service.train_data_path}")
        self.data_service.load_initial_data()

        print(f"ğŸ“¥ æˆåŠŸåŠ è½½ {len(self.data_buffer)} æ¡æ•°æ®")

        for i in range(self.game_batch_num):
            if len(self.data_buffer) < self.batch_size:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                break

            print(f"ğŸ‹ï¸ ç¬¬ {i+1} è½®è®­ç»ƒå¼€å§‹")
            self.policy_update()

            # å®šæœŸä¿å­˜æ¨¡å‹
            if (i + 1) % self.check_freq == 0:
                model_path = f'models/current_policy_batch_{i+1}.model'
                self.policy_value_net.save_model(model_path)
                self.data_service.save_checkpoint(i + 1, model_path)
                print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

        # æœ€ç»ˆä¿å­˜
        if CONFIG['use_frame'] == 'paddle':
            self.policy_value_net.save_model(CONFIG['paddle_model_path'])
            print(f"ğŸ è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {CONFIG['paddle_model_path']}")
        elif CONFIG['use_frame'] == 'pytorch':
            self.policy_value_net.save_model(CONFIG['pytorch_model_path'])
            print(f"ğŸ è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {CONFIG['pytorch_model_path']}")
        else:
            print('ä¸æ”¯æŒæ‰€é€‰æ¡†æ¶')



if __name__ == '__main__':
    init_model = CONFIG.get('init_model_path', None)
    training_pipeline = TrainPipeline(init_model=init_model)

    if CONFIG.get('collect_and_train', False):
        print("ğŸ”„ æ­£åœ¨åŒæ—¶è¿›è¡Œé‡‡é›†å’Œè®­ç»ƒ...")
        from collect_multi_thread import run_pipeline, dynamic_process_manager
        import threading

        def start_collector():
            shared_model = PolicyValueNet(model_file=init_model)
            dynamic_process_manager(shared_model, num_processes=CONFIG['num_processes'])

        collector_thread = threading.Thread(target=start_collector, daemon=True)
        collector_thread.start()

        # ä¸»çº¿ç¨‹è¿è¡Œè®­ç»ƒ
        training_pipeline.run_continuously()
    else:
        print("ğŸ†• åªè¿›è¡Œè®­ç»ƒï¼Œä¸å¯åŠ¨é‡‡é›†")
        training_pipeline.run()
