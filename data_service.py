# data_service.py
import glob
import os
import pickle
import time
from collections import deque
from datetime import timedelta
from multiprocessing import Value

import config
from zip_array import compress_game_data, decompress_game_data
from config import CONFIG

class DataManagementService():
    def __init__(self, ):
        self.use_redis = CONFIG.get('use_redis', False)
        self.use_compression = CONFIG.get('use_data_compression', False)
        self.train_data_path = CONFIG.get('train_data_buffer_path')
        self.buffer_size = CONFIG.get('buffer_size', 100000)
        self.checkpoint_path = CONFIG.get('checkpoint_path', 'checkpoints/latest.pkl')
        self.data_buffer = deque(maxlen=self.buffer_size)
        # åˆå§‹åŒ– Redis å®¢æˆ·ç«¯
        self.redis_client = None
        if self.use_redis:
            import redis
            self.redis_client = redis.Redis(
                host=CONFIG.get('redis_host'),
                port=CONFIG.get('redis_port'),
                db=CONFIG.get('redis_db')
            )

    def load_initial_data(self,data_path= CONFIG.get('train_data_buffer_path')):
        """ä¸€æ¬¡æ€§åŠ è½½åˆå§‹è®­ç»ƒæ•°æ®"""
        if self.use_redis:
            return self._load_from_redis(data_path)
        else:
            return self._load_from_local_file(data_path)

    def refresh_data(self):
        """å¢é‡åˆ·æ–°æ•°æ®ï¼Œé€‚ç”¨äºæŒç»­è®­ç»ƒæ¨¡å¼"""
        if self.use_redis:
            return self._refresh_from_redis()
        else:
            return self._refresh_from_local_file()

    def _load_from_local_file(self,data_path):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å…¨éƒ¨æ•°æ®"""
        try:
            if not os.path.exists(data_path):
                print(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")
                return []

            with open(data_path, 'rb') as f:
                data_dict = pickle.load(f)

            raw_data = data_dict.get('data_buffer', [])
            if self.use_compression:
                raw_data = decompress_game_data(raw_data)

            self.data_buffer.extend(raw_data)
            self.nums = data_dict.get('nums', [])
            self.iters = data_dict.get('iters', 0)
            return self.data_buffer,self.nums,self.iters
        except Exception as e:
            return []

    def _load_from_redis(self,data_path):
        """ä» Redis åŠ è½½æ‰€æœ‰æ•°æ®"""
        try:
            all_data = []
            for key in self.redis_client.keys("train_data:*"):
                item = self.redis_client.get(key)
                if item:
                    data_dict = pickle.loads(item)
                    play_data = data_dict.get('data_buffer', [])
                    all_data.extend(play_data)

            self.data_buffer.extend(all_data)
            self.nums = data_dict.get('nums', [])
            self.iters = data_dict.get('iters', 0)
            return  self.data_buffer,self.nums,self.iters
        except Exception as e:
            return []

    def _refresh_from_local_file(self):
        """å¢é‡æ›´æ–°ï¼šé‡æ–°åŠ è½½æœ¬åœ°æ–‡ä»¶ä¸­æ–°å¢çš„æ•°æ®"""
        try:
            current_len = len(self.data_buffer)
            with open(self.train_data_path, 'rb') as f:
                data_dict = pickle.load(f)

            raw_data = data_dict.get('data_buffer', [])
            if self.use_compression and raw_data :
                raw_data = decompress_game_data(raw_data)

            new_data = raw_data[current_len:]
            self.data_buffer.extend(new_data)
            print(f"ğŸ”„ æ–°å¢åŠ è½½ {len(new_data)} æ¡æ•°æ®ï¼ˆæœ¬åœ°å¢é‡æ›´æ–°ï¼‰")
            return new_data
        except Exception as e:
            print(f"âŒ å¢é‡åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥: {e}")
            return []

    def _refresh_from_redis(self):
        """å¢é‡æ›´æ–°ï¼šä» Redis è·å–æ–°å†™å…¥çš„æ•°æ®"""
        try:
            current_len = len(self.data_buffer)
            new_data = []

            for key in self.redis_client.keys("train_data:*"):
                item = self.redis_client.get(key)
                if item:
                    data_dict = pickle.loads(item)
                    play_data = data_dict.get('data_buffer', [])
                    new_data.extend(play_data)

            existing_set = set(tuple(d) for d in self.data_buffer)
            filtered_new = [d for d in new_data if tuple(d) not in existing_set]
            self.data_buffer.extend(filtered_new)
            print(f"ğŸ”„ æ–°å¢åŠ è½½ {len(filtered_new)} æ¡æ•°æ®ï¼ˆRediså¢é‡æ›´æ–°ï¼‰")
            return filtered_new
        except Exception as e:
            print(f"âŒ Redis å¢é‡åŠ è½½å¤±è´¥: {e}")
            return []

    def save_checkpoint(self, iters, model_path, extra_info=None):
        """ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€åˆ° checkpoint æ–‡ä»¶"""
        checkpoint = {
            'iters': iters,
            'model_path': model_path,
            'timestamp': time.time(),
            'nums': list(self.checkpoint.get('nums', [])),
            'extra': extra_info or {}
        }

        try:
            os.makedirs(os.path.dirname(self.checkpoint_path), exist_ok=True)
            with open(self.checkpoint_path, 'wb') as f:
                pickle.dump(checkpoint, f)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {self.checkpoint_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def load_checkpoint(self):
        """ä» checkpoint æ–‡ä»¶æ¢å¤è®­ç»ƒçŠ¶æ€"""
        if not os.path.exists(self.checkpoint_path):
            print("ğŸ†• æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€")
            return {'iters': 0, 'nums': [], 'model_path': None}

        try:
            with open(self.checkpoint_path, 'rb') as f:
                checkpoint = pickle.load(f)
            print(f"ğŸ” å·²æ¢å¤æ£€æŸ¥ç‚¹ï¼Œè¿­ä»£æ¬¡æ•°: {checkpoint.get('iters', 0)}")
            return checkpoint
        except Exception as e:
            print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return {'iters': 0, 'nums': [], 'model_path': None}

    def get_train_data(self):
        """è·å–å½“å‰è®­ç»ƒæ•°æ®"""
        return list(self.data_buffer)

    def write_play_data(self, process_id, play_data, iters, nums):
        filename = f"data/data_buffer_process_{process_id}.pkl"
        data_to_save = {
            'data_buffer': play_data,
            'iters': iters,
            'nums': nums
        }
        if self.use_compression:
            compressed_data = compress_game_data(play_data)
            data_to_save['data_buffer'] = list(compressed_data)
        try:
            with open(filename + ".tmp", "wb") as f:
                pickle.dump(data_to_save, f)
            os.replace(filename + ".tmp", filename)
            current_total_games = sum(nums) // 2
            print(f"[è¿›ç¨‹ {process_id}] å·²å†™å…¥æœ¬åœ°æ–‡ä»¶: {filename}ï¼Œå½“å‰æ€»å±€æ•°: {current_total_games}")
        except Exception as e:
            print(f"[è¿›ç¨‹ {process_id}] æœ¬åœ°å†™å…¥å¤±è´¥: {e}")


    def merge_all_data(self, output_path, num_processes):
        """åˆå¹¶æ‰€æœ‰é‡‡é›†è¿›ç¨‹æ•°æ®åˆ°ä¸»ç¼“å†²åŒº"""
        merged_buffer = deque(maxlen=self.buffer_size)
        total_iters = 0
        skipped_episodes = 0
        step_nums_unique = []

        existing_episodes = self._load_existing_episodes(output_path)

        global_play_data, global_step_nums, total_iters = self._collect_global_data(num_processes)

        episodes = self._split_into_episodes(global_play_data, global_step_nums)
        unique_episodes = self._deduplicate(episodes)
        final_unique = self._filter_against_existing(unique_episodes, existing_episodes)

        for episode in final_unique:
            compressed = compress_game_data(episode)
            merged_buffer.extend(compressed)
            step_nums_unique.append(len(episode))

        merged_data = {
            'data_buffer': list(merged_buffer),
            'iters': total_iters,
            'nums': step_nums_unique
        }

        try:
            with open(output_path, 'wb') as f:
                pickle.dump(merged_data, f)
            print(f"âœ… æ‰€æœ‰æ•°æ®å·²åˆå¹¶è‡³ {output_path}")
        except Exception as e:
            print(f"âŒ åˆå¹¶æ•°æ®å¤±è´¥: {e}")

        return merged_data

    def _load_existing_episodes(self, path):
        if not os.path.exists(path):
            return set()

        try:
            with open(path, 'rb') as f:
                existing_data = pickle.load(f)
            existing_episodes = set()
            play_index = 0
            step_nums = existing_data.get('nums', [])
            play_data = existing_data.get('data_buffer', [])

            for step_num in step_nums:
                episode_data = play_data[play_index: play_index + step_num]
                if episode_data:
                    hashable = tuple((tuple(s.flatten()), tuple(mp), w) for s, mp, w in episode_data)
                    existing_episodes.add(hashable)
                play_index += step_num
            return existing_episodes
        except Exception as e:
            print(f"âŒ åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
            return set()

    def _collect_global_data(self, num_processes):
        global_play_data = []
        global_step_nums = []
        total_iters = 0

        if self.use_redis:
            for pid in range(num_processes):
                data_key = f'train_data:{pid}'
                item = self.redis_client.get(data_key)
                if item:
                    try:
                        data_dict = pickle.loads(item)
                        play_data = data_dict.get('data_buffer', [])
                        step_nums = data_dict.get('nums', [])
                        pid_iters = data_dict.get('iters', 0)

                        global_play_data.extend(play_data)
                        global_step_nums.extend(step_nums)
                        total_iters += pid_iters
                        self.redis_client.delete(data_key)
                        print(f"[è¿›ç¨‹ {pid}] æ•°æ®å·²ä» Redis å–å‡º")
                    except Exception as e:
                        print(f"âŒ Redis æ•°æ®ååºåˆ—åŒ–å¤±è´¥: {e}")
        else:
            filenames = glob.glob("data/data_buffer_process_*.pkl")
            for filename in filenames:
                if not os.path.exists(filename):
                    continue
                try:
                    with open(filename, 'rb') as f:
                        data_dict = pickle.load(f)

                    play_data = data_dict.get('data_buffer', [])
                    step_nums = data_dict.get('nums', [])
                    pid_iters = data_dict.get('iters', 0)

                    if self.use_compression and play_data :
                        play_data = decompress_game_data(play_data)

                    global_play_data.extend(play_data)
                    global_step_nums.extend(step_nums)
                    total_iters += pid_iters
                    print(f"[å­æ•°æ®é›† {filename}] æ•°æ®å·²ä»æœ¬åœ°å–å‡ºï¼Œæ€»å±€æ•°: {total_iters}")
                except Exception as e:
                    print(f"âŒ åˆå¹¶ {filename} æ—¶å‡ºé”™: {str(e)}")

        return global_play_data, global_step_nums, total_iters

    def _split_into_episodes(self, play_data, step_nums):
        episodes = []
        play_index = 0
        for step_num in step_nums:
            episodes.append(play_data[play_index: play_index + step_num])
            play_index += step_num
        return episodes

    def _deduplicate(self, episodes):
        seen = set()
        unique = []
        # i = 0
        for ep in episodes:
            # print(f"ç¬¬{i}å±€")
            # i = i + 1
            if not ep:
                continue
            hashable = tuple((tuple(s.flatten()), tuple(mp), w) for s, mp, w in ep)
            if hashable not in seen:
                seen.add(hashable)
                unique.append(ep)
        print(f"ğŸ“Œ å­æ•°æ®é›†å†…éƒ¨å»é‡å®Œæˆï¼Œå…±ä¿ç•™ {len(unique)} å±€")
        return unique

    def _filter_against_existing(self, episodes, existing_episodes):
        final_unique = []
        for ep in episodes:
            hashable = tuple((tuple(s.flatten()), tuple(mp), w) for s, mp, w in ep)
            if hashable not in existing_episodes:
                final_unique.append(ep)
        print(f"ğŸ“Œ ä¸ä¸»æ•°æ®å¯¹æ¯”åï¼Œå…±ä¿ç•™ {len(final_unique)} å±€æ–°æ•°æ®")
        return final_unique
